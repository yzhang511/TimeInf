{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q3yTYnwC4ZtR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizi/anaconda3/envs/timeinf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "path_root = '../anomaly_detection/'\n",
    "sys.path.append(str(path_root))\n",
    "\n",
    "from timeinf.utils import block_time_series, sync_time_block_index\n",
    "from detectors import InfluenceFunctionDetector\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_regression_mlp(input_size, output_size) -> nn.Module:\n",
    "    model = torch.nn.Sequential(\n",
    "        nn.Linear(input_size, output_size, bias=True),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataset: data.Dataset,\n",
    "    batch_size: int,\n",
    "    num_train_epochs: int,\n",
    "    learning_rate: float,\n",
    "    weight_decay: float,\n",
    "    seed: int = 0,\n",
    "    disable_tqdm: bool = True,\n",
    ") -> nn.Module:\n",
    "    train_dataloader = data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        total_loss = 0.0\n",
    "        with tqdm(train_dataloader, unit=\"batch\", disable=disable_tqdm) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs)\n",
    "                loss = F.mse_loss(outputs, targets)\n",
    "                total_loss += loss.detach().float()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tepoch.set_postfix(loss=total_loss.item() / len(train_dataloader))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTask(Task):\n",
    "    def compute_train_loss(\n",
    "        self,\n",
    "        batch: Any,\n",
    "        model: nn.Module,\n",
    "        sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        if not sample:\n",
    "            return F.mse_loss(outputs, targets, reduction=\"sum\")\n",
    "        # Sample the outputs from the model's prediction for true Fisher.\n",
    "        with torch.no_grad():\n",
    "            sampled_targets = torch.normal(outputs.detach(), std=math.sqrt(0.5))\n",
    "        return F.mse_loss(outputs, sampled_targets, reduction=\"sum\")\n",
    "\n",
    "    def compute_measurement(\n",
    "        self,\n",
    "        batch: Any,\n",
    "        model: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.compute_train_loss(batch, model, sample=False)\n",
    "\n",
    "    def get_influence_tracked_modules(self) -> Optional[List[str]]:\n",
    "        return None  # Compute influence scores on all available modules.\n",
    "\n",
    "    def get_attention_mask(self, batch: Any) -> Optional[Union[Dict[str, torch.Tensor], torch.Tensor]]:\n",
    "        return None  # Attention mask not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"SMAP_MSL\"\n",
    "data_path = Path(\"../data/multivariate/\") / dataset\n",
    "test_df = pd.read_csv(data_path/\"labeled_anomalies.csv\")\n",
    "smap_df = test_df.loc[test_df.spacecraft == \"SMAP\"]\n",
    "df = smap_df.loc[smap_df.chan_id != \"P-2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.win_size = 100\n",
    "        self.data_path = '../anomaly_detection/data_processed/SMAP'\n",
    "        self.dimensions = [0]\n",
    "        self.dataset = 'SMAP'\n",
    "        self.verbose = False\n",
    "        \n",
    "df = smap_df.loc[smap_df.chan_id != \"P-2\"]\n",
    "config = Config()\n",
    "detector = InfluenceFunctionDetector(config)\n",
    "\n",
    "train_batch_size = 128\n",
    "num_train_epochs = 100\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 1e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test_dict, len_anomaly_dict, len_ratio_dict = {}, {}, {}\n",
    "prec_dict, rec_dict, f1_dict, auc_dict = {}, {}, {}, {}\n",
    "time_dict = {}\n",
    "\n",
    "for channel in df.chan_id.iloc:\n",
    "\n",
    "    if config.dimensions is not None:\n",
    "        ts_test = np.load(data_path/\"test\"/f\"{channel}.npy\")[:,config.dimensions]\n",
    "    else:\n",
    "        ts_test = np.load(data_path/\"test\"/f\"{channel}.npy\")\n",
    "\n",
    "    seq_len = len(ts_test)\n",
    "    anomaly_seqs = df.loc[df.chan_id == channel].anomaly_sequences.to_numpy().item()\n",
    "    anomaly_seqs = re.findall(r'\\d+', anomaly_seqs)\n",
    "    anomaly_intervals = []\n",
    "    for i in list(range(0, len(anomaly_seqs), 2)):\n",
    "        anomaly_intervals.append(anomaly_seqs[i:i+2])\n",
    "    anomaly_intervals = np.array(anomaly_intervals).astype(int)\n",
    "\n",
    "    ground_truth = np.zeros(ts_test.shape[0])\n",
    "    plt.figure(figsize=(16,1))\n",
    "    plt.plot(ts_test, c=\"k\", linewidth=.5)\n",
    "    for anomaly_points in anomaly_intervals:\n",
    "        plt.axvspan(anomaly_points[0], anomaly_points[-1], facecolor='red', alpha=.2)\n",
    "        ground_truth[anomaly_points[0]:anomaly_points[-1]] = 1.\n",
    "\n",
    "    anomaly_len = sum(ground_truth)\n",
    "    anomaly_ratio = anomaly_len / seq_len\n",
    "    print(f\"anomaly ratio is {anomaly_ratio * 100.:.3f} %.\")\n",
    "\n",
    "    len_test_dict.update({channel: seq_len})\n",
    "    len_anomaly_dict.update({channel: anomaly_len})\n",
    "    len_ratio_dict.update({channel: anomaly_ratio})\n",
    "\n",
    "    block_length = 100\n",
    "    X_train, Y_train = block_time_series(ts_test, block_length)\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train).squeeze(), torch.tensor(Y_train)\n",
    "    )\n",
    "\n",
    "    print(f\"start detection for channel {channel} ..\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    n_dim = Y_train.shape[-1]\n",
    "    model = construct_regression_mlp(block_length, n_dim)\n",
    "    model = train(\n",
    "        model,\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        seed=0,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    task = RegressionTask()\n",
    "    model = prepare_model(model, task)\n",
    "    analyzer = Analyzer(\n",
    "        analysis_name=f\"smap_{channel}\",\n",
    "        model=model,\n",
    "        task=task,\n",
    "        cpu=False,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "    analyzer.fit_all_factors(factors_name=f\"smap_{channel}_factor\", dataset=train_dataset)\n",
    "    analyzer.compute_self_scores(\n",
    "        scores_name=f\"smap_{channel}_self_score\",\n",
    "        factors_name=f\"smap_{channel}_factor\",\n",
    "        train_dataset=train_dataset,\n",
    "        overwrite_output_dir=True,\n",
    "    )\n",
    "    self_scores = analyzer.load_self_scores(scores_name=f\"smap_{channel}_self_score\")\n",
    "    anomaly_scores = self_scores[\"all_modules\"]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time, 3)\n",
    "\n",
    "    plt.figure(figsize=(16,1))\n",
    "    loos_viz = anomaly_scores\n",
    "    plt.plot(loos_viz, c=\"k\", linewidth=.5)\n",
    "    for anomaly_points in anomaly_intervals:\n",
    "        plt.axvspan(anomaly_points[0], anomaly_points[-1], facecolor='red', alpha=.2)\n",
    "        ground_truth[anomaly_points[0]:anomaly_points[-1]] = 1.\n",
    "    plt.ylabel(\"anomaly score\")\n",
    "    plt.show()\n",
    "\n",
    "    prec, rec, f1, auc = detector.evaluate(ground_truth[block_length:], anomaly_scores, anomaly_ratio)\n",
    "\n",
    "    prec_dict.update({channel: prec})\n",
    "    rec_dict.update({channel: rec})\n",
    "    f1_dict.update({channel: f1})\n",
    "    auc_dict.update({channel: auc})\n",
    "    time_dict.update({channel: elapsed_time})\n",
    "    \n",
    "    # break\n",
    "\n",
    "smap_metrics = pd.DataFrame({\n",
    "    \"Num_of_Test\": len_test_dict,\n",
    "    \"Len_of_Anomaly\": len_anomaly_dict,\n",
    "    \"Anomaly_Ratio\": len_ratio_dict,\n",
    "    \"Precision\": prec_dict,\n",
    "    \"Recall\": rec_dict,\n",
    "    \"F1\": f1_dict,\n",
    "    \"AUC\": auc_dict,\n",
    "    'Detection_Time(s)': time_dict\n",
    "})\n",
    "\n",
    "smap_metrics.insert(0, \"Dataset\", smap_metrics.index)\n",
    "smap_metrics.reset_index(drop = True, inplace = True)\n",
    "smap_metrics.to_csv('smap_ekfac.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
