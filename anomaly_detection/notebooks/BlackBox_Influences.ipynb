{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961fbc26-c740-493f-8bee-b0d20516a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "module_path = '../'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from detectors import BlackBoxInfluenceFunctionDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0586c6d-ac68-4790-8c34-40a0fdfa2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"SMAP_MSL\"\n",
    "data_path = Path(\"../../data/multivariate/\") / dataset\n",
    "test_df = pd.read_csv(data_path/\"labeled_anomalies.csv\")\n",
    "smap_df = test_df.loc[test_df.spacecraft == \"SMAP\"]\n",
    "# remove P-2 since anomaly transformer also removes this channel\n",
    "df = smap_df.loc[smap_df.chan_id != \"P-2\"]\n",
    "# msl_df = test_df.loc[test_df.spacecraft == \"MSL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255bcfb6-28be-4aef-9f07-966999416fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.win_size = 25\n",
    "        self.data_path = './data_processed/SMAP'\n",
    "        self.dimensions = [0]\n",
    "        self.dataset = 'SMAP'\n",
    "        self.verbose = False\n",
    "        self.black_box_model = \"LSTM\"\n",
    "        self.device = \"cpu\"\n",
    "        self.lr = 1e-3\n",
    "        self.num_epochs = 100\n",
    "        self.weight_decay = 1e-4\n",
    "        self.batch_size = 32\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = 16\n",
    "\n",
    "df = smap_df.loc[smap_df.chan_id != \"P-2\"]\n",
    "config = Config()\n",
    "detector = BlackBoxInfluenceFunctionDetector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975dc1be-91b0-4cee-b618-9d444af8057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly ratio is 8.795 %.\n",
      "start detection for channel P-1 ..\n",
      "block length is 25 time points.\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yizizhang/Desktop/time-series-influence/anomaly_detection/notebooks/../detectors.py:341: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  return torch.tensor(X).to(device), torch.tensor(Y).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 0.2230\n"
     ]
    }
   ],
   "source": [
    "len_test_dict, len_anomaly_dict, len_ratio_dict = {}, {}, {}\n",
    "prec_dict, rec_dict, f1_dict, auc_dict, best_f1_dict = {}, {}, {}, {}, {}\n",
    "prec_adj_dict, rec_adj_dict, f1_adj_dict = {}, {}, {}\n",
    "time_dict = {}\n",
    "\n",
    "for channel in df.chan_id:\n",
    "\n",
    "    if config.dimensions is not None:\n",
    "        ts_test = np.load(data_path/\"test\"/f\"{channel}.npy\")[:,config.dimensions]\n",
    "    else:\n",
    "        ts_test = np.load(data_path/\"test\"/f\"{channel}.npy\")\n",
    "\n",
    "    # import ipdb\n",
    "    # ipdb.set_trace()\n",
    "\n",
    "    seq_len = len(ts_test)\n",
    "    anomaly_seqs = df.loc[df.chan_id == channel].anomaly_sequences.to_numpy().item()\n",
    "    anomaly_seqs = re.findall(r'\\d+', anomaly_seqs)\n",
    "    anomaly_intervals = []\n",
    "    for i in list(range(0, len(anomaly_seqs), 2)):\n",
    "        anomaly_intervals.append(anomaly_seqs[i:i+2])\n",
    "    anomaly_intervals = np.array(anomaly_intervals).astype(int)\n",
    "\n",
    "    ground_truth = np.zeros(ts_test.shape[0])\n",
    "    plt.figure(figsize=(16,1))\n",
    "    plt.plot(ts_test, c=\"k\", linewidth=.5)\n",
    "    for anomaly_points in anomaly_intervals:\n",
    "        plt.axvspan(anomaly_points[0], anomaly_points[-1], facecolor='red', alpha=.2)\n",
    "        ground_truth[anomaly_points[0]:anomaly_points[-1]] = 1.\n",
    "\n",
    "    anomaly_len = sum(ground_truth)\n",
    "    anomaly_ratio = anomaly_len / seq_len\n",
    "    print(f\"anomaly ratio is {anomaly_ratio * 100.:.3f} %.\")\n",
    "\n",
    "    len_test_dict.update({channel: seq_len})\n",
    "    len_anomaly_dict.update({channel: anomaly_len})\n",
    "    len_ratio_dict.update({channel: anomaly_ratio})\n",
    "\n",
    "    print(f\"start detection for channel {channel} ..\")\n",
    "    start_time = time.time()\n",
    "    anomaly_scores = detector.calculate_anomaly_scores(ts = ts_test, channel_id = channel, contamination = min(anomaly_ratio,0.5)) # contamination has to be inthe range (0.0,0.5]\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time, 3)\n",
    "    \n",
    "    plt.figure(figsize=(16,1))\n",
    "    loos_viz = np.ones(len(ts_test)) * np.nan\n",
    "    loos_viz[:len(anomaly_scores)] = anomaly_scores\n",
    "    plt.plot(loos_viz, c=\"k\", linewidth=.5)\n",
    "    for anomaly_points in anomaly_intervals:\n",
    "        plt.axvspan(anomaly_points[0], anomaly_points[-1], facecolor='red', alpha=.2)\n",
    "        ground_truth[anomaly_points[0]:anomaly_points[-1]] = 1.\n",
    "    plt.ylabel(\"anomaly score\")\n",
    "    plt.show()\n",
    "    \n",
    "    detected_outliers = anomaly_scores > np.quantile(anomaly_scores, 1-anomaly_ratio) #\n",
    "    plt.figure(figsize=(16,1))\n",
    "    loos_viz = np.ones(len(ts_test)) * np.nan\n",
    "    loos_viz[:len(detected_outliers)] = detected_outliers\n",
    "    plt.plot(loos_viz, c=\"k\", linewidth=.5)\n",
    "    for anomaly_points in anomaly_intervals:\n",
    "        plt.axvspan(anomaly_points[0], anomaly_points[-1], facecolor='red', alpha=.2)\n",
    "        ground_truth[anomaly_points[0]:anomaly_points[-1]] = 1.\n",
    "    plt.ylabel(\"is outlier?\")\n",
    "    plt.show()\n",
    "    \n",
    "    prec, rec, f1, auc, prec_adj, rec_adj, f1_adj,  best_f1 = detector.evaluate(ground_truth, anomaly_scores, anomaly_ratio)\n",
    "\n",
    "    prec_dict.update({channel: prec})\n",
    "    rec_dict.update({channel: rec})\n",
    "    f1_dict.update({channel: f1})\n",
    "    auc_dict.update({channel: auc})\n",
    "    best_f1_dict.update({channel: best_f1})\n",
    "\n",
    "    time_dict.update({channel: elapsed_time})\n",
    "\n",
    "    prec_adj_dict.update({channel: prec_adj})\n",
    "    rec_adj_dict.update({channel: rec_adj})\n",
    "    f1_adj_dict.update({channel: f1_adj})\n",
    "\n",
    "    break # remove this!\n",
    "\n",
    "smap_metrics = pd.DataFrame({\n",
    "    \"Num_of_Test\": len_test_dict,\n",
    "    \"Len_of_Anomaly\": len_anomaly_dict,\n",
    "    \"Anomaly_Ratio\": len_ratio_dict,\n",
    "    \"Precision(w.o. Adjustment)\": prec_dict,\n",
    "    \"Recall(w.o. Adjustment)\": rec_dict,\n",
    "    \"F1(w.o. Adjustment)\": f1_dict,\n",
    "    \"Precision(w. Adjustment)\": prec_adj_dict,\n",
    "    \"Recall(w. Adjustment)\": rec_adj_dict,\n",
    "    \"F1(w. Adjustment)\": f1_adj_dict,\n",
    "    \"Best_F1_Score\": best_f1_dict,\n",
    "    \"AUC\": auc_dict,\n",
    "    'Detection_Time(s)': time_dict\n",
    "})\n",
    "\n",
    "smap_metrics.insert(0, \"Dataset\", smap_metrics.index)\n",
    "smap_metrics.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e860d9-b7c3-49b7-80f0-d37fe600aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6163-9be2-4c7a-ab18-d5ed9bee7487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4731503-1acd-45e7-8930-3b8b23bd7447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96378b49-8091-47c2-9f0d-4318e2638da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
